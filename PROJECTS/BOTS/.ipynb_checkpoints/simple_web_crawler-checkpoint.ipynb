{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"Web Crawler/Spider\n",
    "\n",
    "This module implements a web crawler. This is very _basic_ only\n",
    "and needs to be extended to do anything usefull with the\n",
    "traversed pages.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import urllib2\n",
    "import urlparse\n",
    "import optparse\n",
    "from cgi import escape\n",
    "from traceback import format_exc\n",
    "from Queue import Queue, Empty as QueueEmpty\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "__version__ = \"0.2\"\n",
    "__copyright__ = \"CopyRight (C) 2008-2011 by James Mills\"\n",
    "__license__ = \"MIT\"\n",
    "__author__ = \"James Mills\"\n",
    "__author_email__ = \"James Mills, James dot Mills st dotred dot com dot au\"\n",
    "\n",
    "USAGE = \"%prog [options] <url>\"\n",
    "VERSION = \"%prog v\" + __version__\n",
    "\n",
    "AGENT = \"%s/%s\" % (__name__, __version__)\n",
    "\n",
    "class Crawler(object):\n",
    "\n",
    "    def __init__(self, root, depth, locked=True):\n",
    "        self.root = root\n",
    "        self.depth = depth\n",
    "        self.locked = locked\n",
    "        self.host = urlparse.urlparse(root)[1]\n",
    "        self.urls = []\n",
    "        self.links = 0\n",
    "        self.followed = 0\n",
    "\n",
    "    def crawl(self):\n",
    "        page = Fetcher(self.root)\n",
    "        page.fetch()\n",
    "        q = Queue()\n",
    "        for url in page.urls:\n",
    "            q.put(url)\n",
    "        followed = [self.root]\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                url = q.get()\n",
    "            except QueueEmpty:\n",
    "                break\n",
    "\n",
    "            n += 1\n",
    "\n",
    "            if url not in followed:\n",
    "                try:\n",
    "                    host = urlparse.urlparse(url)[1]\n",
    "                    if self.locked and re.match(\".*%s\" % self.host, host):\n",
    "                        followed.append(url)\n",
    "                        self.followed += 1\n",
    "                        page = Fetcher(url)\n",
    "                        page.fetch()\n",
    "                        for i, url in enumerate(page):\n",
    "                            if url not in self.urls:\n",
    "                                self.links += 1\n",
    "                                q.put(url)\n",
    "                                self.urls.append(url)\n",
    "                        if n > self.depth and self.depth > 0:\n",
    "                            break\n",
    "                except Exception, e:\n",
    "                    print \"ERROR: Can't process url '%s' (%s)\" % (url, e)\n",
    "                    print format_exc()\n",
    "\n",
    "class Fetcher(object):\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.urls = []\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.urls[x]\n",
    "\n",
    "    def _addHeaders(self, request):\n",
    "        request.add_header(\"User-Agent\", AGENT)\n",
    "\n",
    "    def open(self):\n",
    "        url = self.url\n",
    "        try:\n",
    "            request = urllib2.Request(url)\n",
    "            handle = urllib2.build_opener()\n",
    "        except IOError:\n",
    "            return None\n",
    "        return (request, handle)\n",
    "\n",
    "    def fetch(self):\n",
    "        request, handle = self.open()\n",
    "        self._addHeaders(request)\n",
    "        if handle:\n",
    "            try:\n",
    "                content = unicode(handle.open(request).read(), \"utf-8\",\n",
    "                        errors=\"replace\")\n",
    "                soup = BeautifulSoup(content)\n",
    "                tags = soup('a')\n",
    "            except urllib2.HTTPError, error:\n",
    "                if error.code == 404:\n",
    "                    print >> sys.stderr, \"ERROR: %s -> %s\" % (error, error.url)\n",
    "                else:\n",
    "                    print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            except urllib2.URLError, error:\n",
    "                print >> sys.stderr, \"ERROR: %s\" % error\n",
    "                tags = []\n",
    "            for tag in tags:\n",
    "                href = tag.get(\"href\")\n",
    "                if href is not None:\n",
    "                    url = urlparse.urljoin(self.url, escape(href))\n",
    "                    if url not in self:\n",
    "                        self.urls.append(url)\n",
    "\n",
    "def getLinks(url):\n",
    "    page = Fetcher(url)\n",
    "    page.fetch()\n",
    "    for i, url in enumerate(page):\n",
    "        print \"%d. %s\" % (i, url)\n",
    "\n",
    "def parse_options():\n",
    "    \"\"\"parse_options() -> opts, args\n",
    "\n",
    "    Parse any command-line options given returning both\n",
    "    the parsed options and arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = optparse.OptionParser(usage=USAGE, version=VERSION)\n",
    "\n",
    "    parser.add_option(\"-q\", \"--quiet\",\n",
    "            action=\"store_true\", default=False, dest=\"quiet\",\n",
    "            help=\"Enable quiet mode\")\n",
    "\n",
    "    parser.add_option(\"-l\", \"--links\",\n",
    "            action=\"store_true\", default=False, dest=\"links\",\n",
    "            help=\"Get links for specified url only\")\n",
    "\n",
    "    parser.add_option(\"-d\", \"--depth\",\n",
    "            action=\"store\", type=\"int\", default=30, dest=\"depth\",\n",
    "            help=\"Maximum depth to traverse\")\n",
    "\n",
    "    opts, args = parser.parse_args()\n",
    "\n",
    "    if len(args) < 1:\n",
    "        parser.print_help()\n",
    "        raise SystemExit, 1\n",
    "\n",
    "    return opts, args\n",
    "\n",
    "def main():\n",
    "    opts, args = parse_options()\n",
    "\n",
    "    url = args[0]\n",
    "\n",
    "    if opts.links:\n",
    "        getLinks(url)\n",
    "        raise SystemExit, 0\n",
    "\n",
    "    depth = opts.depth\n",
    "\n",
    "    sTime = time.time()\n",
    "\n",
    "    print \"Crawling %s (Max Depth: %d)\" % (url, depth)\n",
    "    crawler = Crawler(url, depth)\n",
    "    crawler.crawl()\n",
    "    print \"\\n\".join(crawler.urls)\n",
    "\n",
    "    eTime = time.time()\n",
    "    tTime = eTime - sTime\n",
    "\n",
    "    print \"Found:    %d\" % crawler.links\n",
    "    print \"Followed: %d\" % crawler.followed\n",
    "    print \"Stats:    (%d/s after %0.2fs)\" % (\n",
    "            int(math.ceil(float(crawler.links) / tTime)), tTime)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
