{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# links to scrape:\n",
    "### - [ph.com model-page](https://pornhub.com/model/heather-kane/)\n",
    "### - [ph.com hot-videos-page](https://pornhub.com/)\n",
    "### - OTHER PEOPLES VIDEOS (comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import random, fnmatch, logging\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import WebDriverException as WDE\n",
    "from selenium.webdriver.common.proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# IMPORT THE LIBRARY USED TO QUERY A WEBSITE\n",
    "from urllib.request import urlopen\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class phBot(): #{\n",
    "    \n",
    "    # INSTANTIATE CLASS VARIABLES\n",
    "    logging_directory = \"C:/log/here.log\"\n",
    "    \n",
    "    def __init__(self, infile, duration): #{\n",
    "        self.infile = infile\n",
    "        self.duration = duration\n",
    "        print(\"infile == \" + str(self.infile))\n",
    "        print(\"duration == \" + str(duration))\n",
    "    #}\n",
    "    \n",
    "    def watch_videos(self, list_to_watch): #{\n",
    "        pass\n",
    "    #}\n",
    "    \n",
    "    def make_comments(self, link_to_comment): #{\n",
    "        pass\n",
    "    #}\n",
    "    \n",
    "    def check_file(self, infile): #{\n",
    "        print(infile)\n",
    "        print(self.infile)\n",
    "    #}\n",
    "    \n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver(): #{\n",
    "    driver = webdriver.Chrome()\n",
    "    return driver\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teardown_driver(the_driver): #{\n",
    "    # quit/end driver\n",
    "    the_driver.quit()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countdown(n): #{\n",
    "    while n >= 0: #{\n",
    "        print(str(n))\n",
    "        sleep(1)\n",
    "        n -= 1\n",
    "    #}\n",
    "    else: #{\n",
    "        print(\"blast off!\")\n",
    "    #}\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infile == poop\n",
      "duration == 30\n"
     ]
    }
   ],
   "source": [
    "ph_1 = phBot(infile=\"poop\", duration=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poopyHEAD\n",
      "poop\n"
     ]
    }
   ],
   "source": [
    "ph_1.check_file(\"poopyHEAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BeautifulSoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_video_ids_from_url(): #{\n",
    "    url = input(\"Enter a website to extract the ID's from:\")\n",
    "    r = requests.get(\"http://\" + url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data)\n",
    "    for li in soup.find_all('li'): #{\n",
    "        # WRITE TO LOG\n",
    "        logging.info(li.get('id'))\n",
    "    #}\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_image(the_image_file, save_location): #{\n",
    "    pass\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_images_from_url(): #{\n",
    "    url = input(\"Enter a website to extract the IMG's from: \")\n",
    "    r = requests.get(\"http://\" + url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data)\n",
    "    for img in soup.find_all('img'): #{\n",
    "        print(img.get('src'))\n",
    "        # CHECK IF .JPG\n",
    "        # WRITE TO LOG\n",
    "        logging.info(img.get('src'))\n",
    "    #}\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_links_from_url(): #{\n",
    "    # RE-INSTANTIATE GLOBALS\n",
    "    global link_list\n",
    "    url = input(\"Enter a website to extract the URL's from: \")\n",
    "    r = requests.get(\"http://\" + url)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data)\n",
    "    for link in soup.find_all('a'): #{\n",
    "        # WRTIE TO LOG\n",
    "        logging.info(link.get('href'))\n",
    "        # CREATE VARIABLE OF POSSIBLE LINK\n",
    "        soup_link = str(link.get('href'))\n",
    "        # CHECK TO SEE IF VALID LINK\n",
    "        if fnmatch.fnmatch(soup_link, \"*view_video.php?*\"): #{\n",
    "            # IS A LEGIT LINK... now check if fully complete or not\n",
    "            #print(soup_link)\n",
    "            if fnmatch.fnmatch(soup_link, \"*https://www.pornhub.com/*\"): #{\n",
    "                # IT IS A COMPLETE LINK! append to list?\n",
    "                print(soup_link)\n",
    "                link_list.append(soup_link)\n",
    "            #}\n",
    "            else: #{\n",
    "                # ITS NOT COMPLETE! SO COMPLETE IT!\n",
    "                corrected_link = str(\"https://www.pornhub.com\" + soup_link)\n",
    "                print(\"CORRECTED LINK == \" + corrected_link)\n",
    "                link_list.append(soup_link)\n",
    "            #}\n",
    "        #}\n",
    "        else: #{\n",
    "            print(\"NOT LEGIT LINK!\")\n",
    "        #}\n",
    "        \n",
    "    #}\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
