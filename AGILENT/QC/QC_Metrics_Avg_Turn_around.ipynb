{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T20:46:03.332094Z",
     "start_time": "2020-03-25T20:45:59.758044Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pyodbc\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import tempfile\n",
    "from ttkthemes import ThemedStyle\n",
    "import tkinter as tk\n",
    "import tkinter.ttk as ttk\n",
    "from tkinter import messagebox, filedialog, commondialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T20:46:03.449097Z",
     "start_time": "2020-03-25T20:46:03.335095Z"
    },
    "code_folding": [
     37,
     40,
     43,
     60,
     67
    ]
   },
   "outputs": [],
   "source": [
    "class QC_Metrics_AvgTurnAround(): # {\n",
    "    \n",
    "    user_name = str(os.getlogin())\n",
    "    outbound_dir = \"C:/data/outbound/\" + str(pd.Timestamp.now())[:10]\n",
    "    desktop_dir = \"OneDrive - Agilent Technologies/Desktop\"\n",
    "    \n",
    "    def __init__(self, the_logger): # {\n",
    "        self.the_logger = the_logger\n",
    "        # Get/Set USERNAME & DESKTOP DIRECTORIES\n",
    "        self.user_name_dir = os.path.join(\"C:/Users/\", self.user_name)\n",
    "        self.desktop_dir = os.path.join(self.user_name_dir, self.desktop_dir)\n",
    "        print(self.user_name_dir)\n",
    "        print(self.desktop_dir)\n",
    "        self.run(day_range=30)\n",
    "    # }\n",
    "    \n",
    "    def run(self, day_range): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # [2020-02-28]\\\\self.time_unit = time_unit\n",
    "            # [2020-02-28]\\\\self.time_value = time_value\n",
    "            self.day_range = day_range\n",
    "            # get/set current date variable\n",
    "            # [2020-03-06]\\\\the_date = pd.Timestamp.now()\n",
    "            # [2020-03-11]\\\\the_date = pd.Timestamp(ts_input=str(self.end_date.get()))\n",
    "            the_date = pd.Timestamp.now()\n",
    "            # create variable for a month ago\n",
    "            one_month_ago = the_date - timedelta(days = int(day_range))\n",
    "            # [2020-02-28]\\\\\n",
    "            ##one_month_ago = the_date - pd.Timedelta(unit=str(self.time_unit), value=str(self.time_value))\n",
    "            print(type(one_month_ago))\n",
    "            # CREATE METRICS TABLE FROM CLASS METHOD\n",
    "            self.df_metrics_table = self.create_metrics_table()\n",
    "            #############################\n",
    "            # create .csv with no drops #\n",
    "            #############################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_QC_metrics_NO_INDEX-\"\n",
    "                                                      + str(pd.Timestamp.now())[:10]\n",
    "                                                      + \".csv\"), index=True)\n",
    "            \"\"\"\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_metrics_noDrop.csv\"), \n",
    "                                         index=True)\n",
    "            # DROP ROWS WITH PRODUCT LEVEL && QCDATE = NONE\n",
    "            self.df_metrics_table.dropna(axis=0, subset=['QCDate', 'ProductLevel'], how='any', \n",
    "                                         inplace=True)\n",
    "            # create .csv with DROPS\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_metrics_DROP.csv\"), \n",
    "                                         index=True)\n",
    "            \"\"\"\n",
    "            # Set index of METRICS\n",
    "            # [2020-03-11]\\\\self.df_metrics_table.set_index(['QCDate', 'ProductLevel'], inplace=True)\n",
    "            \"\"\"\n",
    "            self.df_metrics_table.index = self.df_metrics_table['QCDate']\n",
    "            del self.df_metrics_table['QCDate']\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # [2020-03-11]\\\\del self.df_metrics_table['ProductLevel']\n",
    "            print(self.df_metrics_table)\n",
    "            # Display Index\n",
    "            print(self.df_metrics_table.index)\n",
    "            #############################\n",
    "            # create .csv with UNSORTED #\n",
    "            #############################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_QC_metrics_UNSORTED.csv\"), \n",
    "                                         index=True)\n",
    "            # SORT INDEX\n",
    "            self.df_metrics_table.sort_index(inplace=True)\n",
    "            ###########################\n",
    "            # create .csv with SORTED #\n",
    "            ###########################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_QC_metrics_SORTED.csv\"), \n",
    "                                         index=True)\n",
    "            print(\"SORTED:\\n\")\n",
    "            print(self.df_metrics_table.index)\n",
    "            \"\"\"\n",
    "            # SLICE & DICE THE DATAFRAME\n",
    "            df_metrics_range = self.df_metrics_table[(self.df_metrics_table['QCDate'] > '2020-01-01') & (self.df_metrics_table['QCDate'] <= '2020-03-01')]\n",
    "            # DETERMINE DAYS IN QC\n",
    "            df_days_in_QC = pd.DataFrame(data=df_metrics_range['PfBatchID'])\n",
    "            calculation = df_metrics_range['QCDate'] - df_metrics_range['AmpDate']\n",
    "            df_days_in_QC['time-in-QC'] = calculation\n",
    "            df_days_in_QC['ProductLevel'] = df_metrics_range['ProductLevel']\n",
    "            df_days_in_QC.to_csv(os.path.join(self.outbound_dir,\n",
    "                                              str(pd.Timestamp.now())[:10] \n",
    "                                              + \"-days-in-QC.csv\"), index=True)\n",
    "            \"\"\"\n",
    "            { PERFORM GROUPBYS HERE }\n",
    "            \"\"\"\n",
    "            df_groupie_1 = df_metrics_range.groupby(['QCDate', 'ProductLevel'])[['PfBatchID']]\n",
    "            df_groupie_1_ct = df_groupie_1.count()\n",
    "            df_groupie_1_avg = df_metrics_range.mean()\n",
    "            \"\"\"\n",
    "            { PERFORM GROUPBY HERE }\n",
    "            \"\"\"\n",
    "            x = df_metrics_range.groupby([\"ProductLevel\"])[[\"QCDate\"]]\n",
    "            y = df_metrics_range.groupby([\"QCDate\"])[[\"ProductLevel\"]]\n",
    "            df_x = pd.DataFrame(data=x.mean())\n",
    "            df_x.to_csv(os.path.join(self.outbound_dir, \n",
    "                                         str(pd.Timestamp.now())[:10]\n",
    "                                         + \"-x-mean.csv\"), index=True)\n",
    "            df_y = pd.DataFrame(data=y.mean())\n",
    "            df_y.to_csv(os.path.join(self.outbound_dir,\n",
    "                                         str(pd.Timestamp.now())[:10]\n",
    "                                         + \"-y-mean.csv\"), index=True)\n",
    "            # HOW MANY PfBatchID per ProductLevel, per QCDate?\n",
    "            self.ProdsPerDay = df_metrics_range.groupby(['QCDate', 'ProductLevel'])[['PfBatchID']].count()\n",
    "            df_prods_per_day = pd.DataFrame(data=self.ProdsPerDay)\n",
    "            self.ProdsPerDay.to_csv(os.path.join(self.outbound_dir,\n",
    "                                                 str(pd.Timestamp.now())[:10]\n",
    "                                                 + \"-ProdsPerDay.csv\"), index=True)\n",
    "            \"\"\"\n",
    "            { PERFORM DATA-AGGREGATION HERE }\n",
    "            { PERFORM DATA-AGGREGATION HERE }\n",
    "            \"\"\"\n",
    "            # rolling window?\n",
    "            # [2020-03-25]\\\\r = df_groupie_1.rolling(window=3).mean()\n",
    "            # AVERAGE FOR EACH ROW\n",
    "            av_column = df_metrics_range.groupby(['ProductLevel', pd.Grouper(key='QCDate', freq='D')])['ProductNo'].mean()\n",
    "            print(av_column)\n",
    "            av_column.to_csv(os.path.join(self.outbound_dir, \"av-column-\"\n",
    "                                          + str(pd.Timestamp.now())[:10]\n",
    "                                          + \".csv\"), index=True)\n",
    "            # [2020-03-24]\\\\print(r['PfBatchID'].aggregate(['mean']))\n",
    "            #print(r.agg({'PfBatchID':'mean'}))\n",
    "            \"\"\"\n",
    "            { PERFORM RESAMPLE HERE }\n",
    "            \"\"\"\n",
    "            # df_grouper = df_metrics_range.set_index('QCDate').groupby('ProductLevel')[\"PfBatchID\"].resample('D').mean()\n",
    "            \"\"\"\n",
    "            { PERFORM RESAMPLE HERE }\n",
    "            \"\"\"\n",
    "            ##########\n",
    "            # EXPORT #\n",
    "            ##########\n",
    "            df_groupie_1.to_csv(os.path.join(self.outbound_dir, \"df-groupie-1-\"\n",
    "                                             + str(pd.Timestamp.now())[:10]\n",
    "                                             + \".csv\"), index=True)\n",
    "            df_groupie_1_ct.to_csv(os.path.join(self.outbound_dir, \"df-groupie-1-ct-\"\n",
    "                                                + str(pd.Timestamp.now())[:10]\n",
    "                                                + \".csv\"), index=True)\n",
    "            df_groupie_1_avg.to_csv(os.path.join(self.outbound_dir, \"df-groupie-1-avg-\"\n",
    "                                                 + str(pd.Timestamp.now())[:10]\n",
    "                                                 + \".csv\"), index=True)\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    def perform_groupBy(self, dataframe): # { \n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            pass\n",
    "        # }\n",
    "        except: # {\n",
    "            pass\n",
    "        # }\n",
    "        else: # {\n",
    "            pass\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    \"\"\"\n",
    "    Referred to as \"ProdflowII\" in SQL-Server\n",
    "    \"\"\"\n",
    "    def pull_ProdflowII_table(self, table_name): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # CREATION CONNECTION STR\n",
    "            conn_str = str(\n",
    "                r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                r'SERVER=wtkngappflow1.is.agilent.net;'\n",
    "                r'DATABASE=ProdFlowII_Prod;'\n",
    "                r'Trusted_Connection=yes;'\n",
    "            )\n",
    "            # CREATE PYODBC CONNECTION\n",
    "            cnxn_ProdflowII = pyodbc.connect(conn_str)\n",
    "            # [2020-02-028]\\\\crsr_ProdflowII = cnxn_ProdflowII.cursor()\n",
    "            # PERFORM SQL QUERY AND SET AS DATAFRAME\n",
    "            df_ProdflowII_table = pd.read_sql_query(sql='SELECT * FROM ' + str(table_name),\n",
    "                                                    con=cnxn_ProdflowII\n",
    "                                                    )\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_ProdflowII_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    \"\"\"\n",
    "    Referred to as \"Prodflow\" in SQL-Server\n",
    "    \"\"\"\n",
    "    def pull_ProdflowIII_table(self, table_name): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # CREATE CONNECTION STRING\n",
    "            conn_str = str(\n",
    "                r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                r'SERVER=wtkngappflow1.is.agilent.net;'\n",
    "                r'DATABASE=ProdFlow;'\n",
    "                r'Trusted_Connection=yes;'\n",
    "            )\n",
    "            # CREATE PYODBC CONNECTION\n",
    "            cnxn_ProdflowIII = pyodbc.connect(conn_str)\n",
    "            # [2020-02-28]\\\\crsr_ProdflowIII = cnxn_ProdflowIII,cursor()\n",
    "            # PERFORM SQL QUERY AND SET AS DATAFRAME\n",
    "            df_ProdflowIII_table = pd.read_sql_query(sql='SELECT * FROM ' + str(table_name),\n",
    "                                                     con=cnxn_ProdflowIII,\n",
    "                                                     parse_dates=['QCDate']\n",
    "                                                     )\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_ProdflowIII_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    def create_metrics_table(self): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # pull PRODUCTS table\n",
    "            self.df_products = self.pull_ProdflowII_table(table_name='Products')\n",
    "            # RENAME ['Product#'] column to [\"ProductNo\"]\n",
    "            self.df_products.rename(columns={'Product#': 'ProductNo'}, inplace=True)\n",
    "            self.df_products.to_csv(os.path.join(self.outbound_dir, \"df-PRODUCTS-table-\" \n",
    "                                                + str(pd.Timestamp.now())[:10]\n",
    "                                                + \".csv\"), index=False)\n",
    "            print(\"\\tORDERS-TABLE:\\n\" + str(self.df_products))\n",
    "            # pull tblProdflow TABLE\n",
    "            self.df_tblProdflow = self.pull_ProdflowIII_table(table_name='tblProdflow')\n",
    "            \"\"\"\n",
    "            self.df_tblProdflow.to_csv(os.path.join(self.outbound_dir, \"df-tblProdflow-\"\n",
    "                                                    + str(pd.Timestamp.now())[:10] \n",
    "                                                    + \".csv\"), index=False)\n",
    "            \"\"\"\n",
    "            ########################\n",
    "            # CREATE METRICS TABLE #\n",
    "            ########################\n",
    "            df_metrics_table = pd.merge(self.df_products, self.df_tblProdflow, on='ProductNo', how='right')\n",
    "            ######################\n",
    "            # CHANGE DATA TYPES: #\n",
    "            ######################\n",
    "            # CHANGE ['QCDate'] COLUMN TO DATETIME\n",
    "            df_metrics_table['QCDate'] = pd.to_datetime(df_metrics_table['QCDate'])\n",
    "            # SAME WITH OTHER COLUMNS\n",
    "            df_metrics_table['AmpDate'] = pd.to_datetime(df_metrics_table['AmpDate'])\n",
    "            df_metrics_table['OriginationDate'] = pd.to_datetime(df_metrics_table['OriginationDate'])\n",
    "            df_metrics_table['EntryDate'] = pd.to_datetime(df_metrics_table['EntryDate'])\n",
    "            # CHANGE ['ProductLevel'] COLUMN TO FLOAT\n",
    "            # [2020-03-11]\\\\df_metrics_table['ProductLevel'] = df_metrics_table['ProductLevel'].values.astype(float)\n",
    "            # [2020-03-11]\\\\df_metrics_table.ProductLevel = df_metrics_table.astype(float)\n",
    "            # [2020-03-11]\\\\df_metrics_table['ProductLevel'] = df_metrics_table['ProductLevel'].values.astype(int)\n",
    "            \"\"\"\n",
    "            # CHANGE 'ProductLevel' to CATEGORICAL\n",
    "            df_metrics_table['ProductLevel'] = pd.Categorical(df_metrics_table['ProductLevel'],\n",
    "                                                          categories=[1, 2, 3], ordered=False)\n",
    "            \"\"\"\n",
    "            #####################################################\n",
    "            # DROP ALL ROWS WITHOUT A 'QCDATE' & 'ProductLevel' #\n",
    "            #####################################################\n",
    "            df_metrics_table.dropna(axis=0, subset=['QCDate', 'ProductLevel'], \n",
    "                                    how='any', inplace=True)\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_metrics_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T20:46:30.177489Z",
     "start_time": "2020-03-25T20:46:03.452100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/derbates\n",
      "C:/Users/derbates\\OneDrive - Agilent Technologies/Desktop\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "Operation Completed Successfully...\n",
      "\tORDERS-TABLE:\n",
      "        ProductID       ProductNo                                 ProductName  \\\n",
      "0            5417     00301-23032                       Benzophenone Solution   \n",
      "1            5418    00301-23058A  Octafluoronaphthalene Calibration Standard   \n",
      "2            5419    00301-23058B  Octafluoronaphthalene Calibration Standard   \n",
      "3            5420     01080-68703                             Gradient Sample   \n",
      "4            5421   01080-68704-1                         LC Isocratic Sample   \n",
      "...           ...             ...                                         ...   \n",
      "108562      67040  ZWAT042881SA-6       Detector Linearity Test Solution  Kit   \n",
      "108563      67041  ZWAT042881SA-7       Detector Linearity Test Solution  Kit   \n",
      "108564      67019  ZWAT042885SA-1           Wavelength Accuracy Test Solution   \n",
      "108565      67042    ZWAT042887SA             Autosampler Validation Solution   \n",
      "108566      50108   ZX-TESTMIX-01                    GC x GC Testing Solution   \n",
      "\n",
      "        LineID Description      Specials OriginationDate  EntryDate Status  \\\n",
      "0            7        None   00301-23032      1999-01-20 2011-05-09    oem   \n",
      "1            1        None  00301-23058A      1999-01-20 2011-05-09    oem   \n",
      "2            7        None  00301-23058B      1999-01-20 2011-05-09    oem   \n",
      "3            7        None   01080-68703      1998-02-04 2011-05-09    oem   \n",
      "4            7        None   01080-68704      1995-12-18 2011-05-09    oem   \n",
      "...        ...         ...           ...             ...        ...    ...   \n",
      "108562       7        None          None      2013-09-11 2013-09-11   None   \n",
      "108563       7        None          None      2013-09-11 2013-09-11   None   \n",
      "108564       7        None          None      2013-09-10 2013-09-10   None   \n",
      "108565       7        None          None      2013-09-11 2013-09-11   None   \n",
      "108566       7        None          None      2011-04-19 2011-05-09   None   \n",
      "\n",
      "       MatrixNumber  ...  MatrixNotes         RecipeId RushPriority IsVoided  \\\n",
      "0           JHP-014  ...         None  RP-00106               False     VOID   \n",
      "1           JHP-014  ...         None  RP-00107               False     VOID   \n",
      "2           JHP-014  ...         None  RP-00108               False     VOID   \n",
      "3           JHP-021  ...         None  RP-00109               False            \n",
      "4           JHP-021  ...         None  RP-00110               False     VOID   \n",
      "...             ...  ...          ...              ...          ...      ...   \n",
      "108562      JHP-021  ...         None  RP-39850               False     None   \n",
      "108563      JHP-021  ...         None  RP-29307               False     None   \n",
      "108564      JHP-040  ...         None  RP-32057               False     None   \n",
      "108565      JHP-028  ...         None  RP-29112               False     None   \n",
      "108566      JHP-031  ...         None  RP-03945               False            \n",
      "\n",
      "        SagePartNumber ProductLevel  DateInserted CofATemplate  \\\n",
      "0          00301-23032          NaN           NaT         None   \n",
      "1                 None          NaN           NaT         None   \n",
      "2                 None          NaN           NaT         None   \n",
      "3                 None          2.0           NaT         None   \n",
      "4        01080-68704-1          2.0           NaT      GENERIC   \n",
      "...                ...          ...           ...          ...   \n",
      "108562  ZWAT042881SA-6          1.0           NaT      GENERIC   \n",
      "108563  ZWAT042881SA-7          1.0           NaT      GENERIC   \n",
      "108564    ZWAT042885SA          2.0           NaT         None   \n",
      "108565    ZWAT042887SA          1.0           NaT      GENERIC   \n",
      "108566   ZX-TESTMIX-01          2.0           NaT      GENERIC   \n",
      "\n",
      "       UniqueRequestID                    Test_Timestamp  \n",
      "0                 None     b'\\x00\\x00\\x00\\x00\\x00!\\xe4c'  \n",
      "1                 None  b'\\x00\\x00\\x00\\x00\\x00#\\xa1\\x1d'  \n",
      "2                 None     b'\\x00\\x00\\x00\\x00\\x00!\\xe4e'  \n",
      "3                 None     b'\\x00\\x00\\x00\\x00\\x00!\\xe4f'  \n",
      "4                 None     b'\\x00\\x00\\x00\\x00\\x00!\\xe4g'  \n",
      "...                ...                               ...  \n",
      "108562            None  b'\\x00\\x00\\x00\\x00\\x00#\\x8d\\xa5'  \n",
      "108563            None  b'\\x00\\x00\\x00\\x00\\x00#\\x8d\\xa6'  \n",
      "108564            None  b'\\x00\\x00\\x00\\x00\\x00#\\x8d\\xa7'  \n",
      "108565            None  b'\\x00\\x00\\x00\\x00\\x00#\\x8d\\xa8'  \n",
      "108566            None  b'\\x00\\x00\\x00\\x00\\x00#\\x8d\\xa9'  \n",
      "\n",
      "[108567 rows x 42 columns]\n",
      "Operation Completed Successfully...\n",
      "Operation Completed Successfully...\n",
      "\n",
      "TYPE : <class 'pandas.core.base.DataError'>\n",
      "FILE : <ipython-input-2-a96f57b3fee5>\n",
      "LINE : 97\n",
      "MESG : \n",
      "\n",
      "<class 'pandas.core.base.DataError'>\n",
      "No numeric types to aggregate\n",
      "\t\t<traceback object at 0x0000023FDFAF5C08>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": # {\n",
    "    test_metrics = QC_Metrics_AvgTurnAround(the_logger=None)\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
