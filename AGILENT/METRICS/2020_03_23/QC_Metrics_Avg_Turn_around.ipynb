{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T18:16:58.962024Z",
     "start_time": "2020-03-23T18:16:53.036344Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pyodbc\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import tempfile\n",
    "from ttkthemes import ThemedStyle\n",
    "import tkinter as tk\n",
    "import tkinter.ttk as ttk\n",
    "from tkinter import messagebox, filedialog, commondialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T18:16:59.080035Z",
     "start_time": "2020-03-23T18:16:58.966070Z"
    },
    "code_folding": [
     37,
     40,
     43,
     60,
     67
    ]
   },
   "outputs": [],
   "source": [
    "class QC_Metrics_AvgTurnAround(): # {\n",
    "    \n",
    "    user_name = str(os.getlogin())\n",
    "    outbound_dir = \"C:/data/outbound/\" + str(pd.Timestamp.now())[:10]\n",
    "    desktop_dir = \"OneDrive - Agilent Technologies/Desktop\"\n",
    "    \n",
    "    def __init__(self, the_logger): # {\n",
    "        self.the_logger = the_logger\n",
    "        # Get/Set USERNAME & DESKTOP DIRECTORIES\n",
    "        self.user_name_dir = os.path.join(\"C:/Users/\", self.user_name)\n",
    "        self.desktop_dir = os.path.join(self.user_name_dir, self.desktop_dir)\n",
    "        print(self.user_name_dir)\n",
    "        print(self.desktop_dir)\n",
    "        self.run(day_range=30)\n",
    "    # }\n",
    "    \n",
    "    def run(self, day_range): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # [2020-02-28]\\\\self.time_unit = time_unit\n",
    "            # [2020-02-28]\\\\self.time_value = time_value\n",
    "            self.day_range = day_range\n",
    "            # get/set current date variable\n",
    "            # [2020-03-06]\\\\the_date = pd.Timestamp.now()\n",
    "            # [2020-03-11]\\\\the_date = pd.Timestamp(ts_input=str(self.end_date.get()))\n",
    "            the_date = pd.Timestamp.now()\n",
    "            # create variable for a month ago\n",
    "            one_month_ago = the_date - timedelta(days = int(day_range))\n",
    "            # [2020-02-28]\\\\\n",
    "            ##one_month_ago = the_date - pd.Timedelta(unit=str(self.time_unit), value=str(self.time_value))\n",
    "            print(type(one_month_ago))\n",
    "            # CREATE METRICS TABLE FROM CLASS METHOD\n",
    "            self.df_metrics_table = self.create_metrics_table()\n",
    "            #############################\n",
    "            # create .csv with no drops #\n",
    "            #############################\n",
    "            \"\"\"\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_metrics_noDrop.csv\"), \n",
    "                                         index=True)\n",
    "            # DROP ROWS WITH PRODUCT LEVEL && QCDATE = NONE\n",
    "            self.df_metrics_table.dropna(axis=0, subset=['QCDate', 'ProductLevel'], how='any', \n",
    "                                         inplace=True)\n",
    "            # create .csv with DROPS\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_metrics_DROP.csv\"), \n",
    "                                         index=True)\n",
    "            \"\"\"\n",
    "            # Set index of METRICS\n",
    "            # [2020-03-11]\\\\self.df_metrics_table.set_index(['QCDate', 'ProductLevel'], inplace=True)\n",
    "            \"\"\"\n",
    "            self.df_metrics_table.index = self.df_metrics_table['QCDate']\n",
    "            del self.df_metrics_table['QCDate']\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # [2020-03-11]\\\\del self.df_metrics_table['ProductLevel']\n",
    "            print(self.df_metrics_table)\n",
    "            # Display Index\n",
    "            print(self.df_metrics_table.index)\n",
    "            #############################\n",
    "            # create .csv with UNSORTED #\n",
    "            #############################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_QC_metrics_UNSORTED.csv\"), \n",
    "                                         index=True)\n",
    "            # SORT INDEX\n",
    "            self.df_metrics_table.sort_index(inplace=True)\n",
    "            ###########################\n",
    "            # create .csv with SORTED #\n",
    "            ###########################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_QC_metrics_SORTED.csv\"), \n",
    "                                         index=True)\n",
    "            print(\"SORTED:\\n\")\n",
    "            print(self.df_metrics_table.index)\n",
    "            \"\"\"\n",
    "            # SLICE & DICE THE DATAFRAME\n",
    "            df_metrics_range = self.df_metrics_table[(self.df_metrics_table['QCDate'] > '2020-01-01') & (self.df_metrics_table['QCDate'] <= '2020-03-01')]\n",
    "            # DETERMINE DAYS IN QC\n",
    "            df_days_in_QC = pd.DataFrame(data=df_metrics_range['PfBatchID'])\n",
    "            calculation = df_metrics_range['QCDate'] - df_metrics_range['AmpDate']\n",
    "            df_days_in_QC['time-in-QC'] = calculation\n",
    "            df_days_in_QC['ProductLevel'] = df_metrics_range['ProductLevel']\n",
    "            df_days_in_QC.to_csv(\"C:/data/Outbound/2020-03-23/\" \n",
    "                                 + str(pd.Timestamp.now())[:10] \n",
    "                                 + \"-days-in-QC.csv\")\n",
    "            \"\"\"\n",
    "            { PERFORM GROUPBYS HERE }\n",
    "            { PERFORM GROUPBY HERE }\n",
    "            \"\"\"\n",
    "            # HOW MANY PfBatchID per ProductLevel, per QCDate?\n",
    "            self.ProdsPerDay = df_metrics_range.groupby(['QCDate', 'ProductLevel'])[['PfBatchID']].count()\n",
    "            df_prods_per_day = pd.DataFrame(data=self.ProdsPerDay)\n",
    "            self.ProdsPerDay.to_csv(\"c:/data/Outbound/2020-03-23/\"\n",
    "                                    + str(pd.Timestamp.now())[:10]\n",
    "                                    + \"-ProdsPerDay.csv\")\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    \"\"\"\n",
    "    Referred to as \"ProdflowII\" in SQL-Server\n",
    "    \"\"\"\n",
    "    def pull_ProdflowII_table(self, table_name): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # CREATION CONNECTION STR\n",
    "            conn_str = str(\n",
    "                r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                r'SERVER=wtkngappflow1.is.agilent.net;'\n",
    "                r'DATABASE=ProdFlowII_Prod;'\n",
    "                r'Trusted_Connection=yes;'\n",
    "            )\n",
    "            # CREATE PYODBC CONNECTION\n",
    "            cnxn_ProdflowII = pyodbc.connect(conn_str)\n",
    "            # [2020-02-028]\\\\crsr_ProdflowII = cnxn_ProdflowII.cursor()\n",
    "            # PERFORM SQL QUERY AND SET AS DATAFRAME\n",
    "            df_ProdflowII_table = pd.read_sql_query(sql='SELECT * FROM ' + str(table_name),\n",
    "                                                    con=cnxn_ProdflowII\n",
    "                                                    )\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_ProdflowII_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    \"\"\"\n",
    "    Referred to as \"Prodflow\" in SQL-Server\n",
    "    \"\"\"\n",
    "    def pull_ProdflowIII_table(self, table_name): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # CREATE CONNECTION STRING\n",
    "            conn_str = str(\n",
    "                r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                r'SERVER=wtkngappflow1.is.agilent.net;'\n",
    "                r'DATABASE=ProdFlow;'\n",
    "                r'Trusted_Connection=yes;'\n",
    "            )\n",
    "            # CREATE PYODBC CONNECTION\n",
    "            cnxn_ProdflowIII = pyodbc.connect(conn_str)\n",
    "            # [2020-02-28]\\\\crsr_ProdflowIII = cnxn_ProdflowIII,cursor()\n",
    "            # PERFORM SQL QUERY AND SET AS DATAFRAME\n",
    "            df_ProdflowIII_table = pd.read_sql_query(sql='SELECT * FROM ' + str(table_name),\n",
    "                                                     con=cnxn_ProdflowIII,\n",
    "                                                     parse_dates=['QCDate']\n",
    "                                                     )\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_ProdflowIII_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    def create_metrics_table(self): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # pull PRODUCTS table\n",
    "            self.df_products = self.pull_ProdflowII_table(table_name='Products')\n",
    "            # RENAME ['Product#'] column to [\"ProductNo\"]\n",
    "            self.df_products.rename(columns={'Product#': 'ProductNo'}, inplace=True)\n",
    "            self.df_products.to_csv(os.path.join(self.outbound_dir, \"df-PRODUCTS-table-\" \n",
    "                                                + str(pd.Timestamp.now())[:10]\n",
    "                                                + \".csv\"), index=False)\n",
    "            print(\"\\tORDERS-TABLE:\\n\" + str(self.df_products.info()))\n",
    "            # pull tblProdflow TABLE\n",
    "            self.df_tblProdflow = self.pull_ProdflowIII_table(table_name='tblProdflow')\n",
    "            self.df_tblProdflow.to_csv(os.path.join(self.outbound_dir, \"df-tblProdflow-\"\n",
    "                                                    + str(pd.Timestamp.now())[:10] \n",
    "                                                    + \".csv\"), index=False)\n",
    "            print(self.df_tblProdflow.info())\n",
    "            ########################\n",
    "            # CREATE METRICS TABLE #\n",
    "            ########################\n",
    "            df_metrics_table = pd.merge(self.df_products, self.df_tblProdflow, on='ProductNo', how='right')\n",
    "            ######################\n",
    "            # CHANGE DATA TYPES: #\n",
    "            ######################\n",
    "            # CHANGE ['QCDate'] COLUMN TO DATETIME\n",
    "            df_metrics_table['QCDate'] = pd.to_datetime(df_metrics_table['QCDate'])\n",
    "            # SAME WITH OTHER COLUMNS\n",
    "            df_metrics_table['AmpDate'] = pd.to_datetime(df_metrics_table['AmpDate'])\n",
    "            df_metrics_table['OriginationDate'] = pd.to_datetime(df_metrics_table['OriginationDate'])\n",
    "            df_metrics_table['EntryDate'] = pd.to_datetime(df_metrics_table['EntryDate'])\n",
    "            # CHANGE ['ProductLevel'] COLUMN TO FLOAT\n",
    "            # [2020-03-11]\\\\df_metrics_table['ProductLevel'] = df_metrics_table['ProductLevel'].values.astype(float)\n",
    "            # [2020-03-11]\\\\df_metrics_table.ProductLevel = df_metrics_table.astype(float)\n",
    "            # [2020-03-11]\\\\df_metrics_table['ProductLevel'] = df_metrics_table['ProductLevel'].values.astype(int)\n",
    "            \"\"\"\n",
    "            # CHANGE 'ProductLevel' to CATEGORICAL\n",
    "            df_metrics_table['ProductLevel'] = pd.Categorical(df_metrics_table['ProductLevel'],\n",
    "                                                          categories=[1, 2, 3], ordered=False)\n",
    "            \"\"\"\n",
    "            #####################################################\n",
    "            # DROP ALL ROWS WITHOUT A 'QCDATE' & 'ProductLevel' #\n",
    "            #####################################################\n",
    "            df_metrics_table.dropna(axis=0, subset=['QCDate', 'ProductLevel'], \n",
    "                                    how='any', inplace=True)\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_metrics_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T18:17:33.273537Z",
     "start_time": "2020-03-23T18:16:59.087033Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/derbates\n",
      "C:/Users/derbates\\OneDrive - Agilent Technologies/Desktop\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "Operation Completed Successfully...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108544 entries, 0 to 108543\n",
      "Data columns (total 42 columns):\n",
      "ProductID          108544 non-null int64\n",
      "ProductNo          108544 non-null object\n",
      "ProductName        108516 non-null object\n",
      "LineID             108544 non-null int64\n",
      "Description        5976 non-null object\n",
      "Specials           2443 non-null object\n",
      "OriginationDate    108150 non-null datetime64[ns]\n",
      "EntryDate          108542 non-null datetime64[ns]\n",
      "Status             6036 non-null object\n",
      "MatrixNumber       107659 non-null object\n",
      "Components         108544 non-null int64\n",
      "Availability       4843 non-null object\n",
      "EquivalentSoln     103100 non-null object\n",
      "EquivalentNeat     103258 non-null object\n",
      "Packaging          107585 non-null object\n",
      "Storage            100898 non-null object\n",
      "ExpirationDate     108519 non-null float64\n",
      "ShipType           95291 non-null object\n",
      "UN#                93608 non-null object\n",
      "ShipClass          87515 non-null object\n",
      "PackGrp            87397 non-null object\n",
      "ShipSpecial        83172 non-null object\n",
      "Hazard1            2475 non-null object\n",
      "Hazard2            4258 non-null object\n",
      "Hazard3            124 non-null object\n",
      "Prop65             6140 non-null object\n",
      "MSDS               6138 non-null object\n",
      "Analysis           5696 non-null object\n",
      "CofAHeader         102360 non-null float64\n",
      "Agilent#           694 non-null object\n",
      "Tedia#             2330 non-null object\n",
      "ProdNotes          94024 non-null object\n",
      "MatrixNotes        9197 non-null object\n",
      "RecipeId           81196 non-null object\n",
      "RushPriority       108544 non-null bool\n",
      "IsVoided           93504 non-null object\n",
      "SagePartNumber     32234 non-null object\n",
      "ProductLevel       91496 non-null float64\n",
      "DateInserted       38 non-null datetime64[ns]\n",
      "CofATemplate       79518 non-null object\n",
      "UniqueRequestID    5 non-null object\n",
      "Test_Timestamp     108544 non-null object\n",
      "dtypes: bool(1), datetime64[ns](3), float64(3), int64(3), object(32)\n",
      "memory usage: 34.1+ MB\n",
      "\tORDERS-TABLE:\n",
      "None\n",
      "Operation Completed Successfully...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109204 entries, 0 to 109203\n",
      "Data columns (total 53 columns):\n",
      "PfIDNo              109204 non-null int64\n",
      "PfBatchID           109204 non-null object\n",
      "ProductNo           109185 non-null object\n",
      "OrdID               93072 non-null float64\n",
      "QuoteNo             109168 non-null object\n",
      "PfSentTo            109139 non-null object\n",
      "PrepDate            108184 non-null datetime64[ns]\n",
      "PrepVolume          109159 non-null float64\n",
      "PrepUnit            109128 non-null object\n",
      "PrepVessel          109125 non-null float64\n",
      "PrepVBarcode        58516 non-null object\n",
      "PrdSaleUnit         89893 non-null object\n",
      "PrepMatrixNo        109129 non-null object\n",
      "PrepMatrixLot       108260 non-null object\n",
      "PrepInits           108019 non-null object\n",
      "PrepMemo            61684 non-null object\n",
      "PrepNotebookRef     48969 non-null object\n",
      "BulkQCStatus        24 non-null object\n",
      "BulkQCDate          75462 non-null datetime64[ns]\n",
      "BulkPassFail        74538 non-null object\n",
      "BulkQCInits         75446 non-null object\n",
      "BulkQCMemo          2152 non-null object\n",
      "BulkLotNo           73793 non-null object\n",
      "AmpRetains          52037 non-null float64\n",
      "AmpDate             93169 non-null datetime64[ns]\n",
      "AmpPreLabel         109204 non-null bool\n",
      "AmpNumberGood       93496 non-null float64\n",
      "AmpNumberBad        77789 non-null float64\n",
      "AmpBulkRemain       4381 non-null float64\n",
      "AmpTimeIn           92926 non-null datetime64[ns]\n",
      "AmpTimeOut          92926 non-null datetime64[ns]\n",
      "AmpInits            93134 non-null object\n",
      "QCStatus            573 non-null object\n",
      "QCDate              103640 non-null datetime64[ns]\n",
      "QCPassFail          103829 non-null object\n",
      "QCAuthInits         103628 non-null object\n",
      "QCMemo              55549 non-null object\n",
      "QCMethod            1205 non-null object\n",
      "LotNo               100921 non-null object\n",
      "QCExpMonth          93846 non-null float64\n",
      "QCExpDate           92256 non-null datetime64[ns]\n",
      "QCSellBy            0 non-null object\n",
      "QCFullValidation    109204 non-null bool\n",
      "QCValidation        109170 non-null float64\n",
      "FgInvCount          78363 non-null float64\n",
      "FgAccpacNote        19740 non-null object\n",
      "CofAHeaderNo        109172 non-null float64\n",
      "QCChromatogram      0 non-null object\n",
      "Correct             109204 non-null bool\n",
      "DoNotCorrect        109204 non-null bool\n",
      "DensityUnit         0 non-null object\n",
      "DensityTemp         0 non-null object\n",
      "recipeid            1 non-null object\n",
      "dtypes: bool(4), datetime64[ns](7), float64(11), int64(1), object(30)\n",
      "memory usage: 41.2+ MB\n",
      "None\n",
      "Operation Completed Successfully...\n",
      "Operation Completed Successfully...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": # {\n",
    "    test_metrics = QC_Metrics_AvgTurnAround(the_logger=None)\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
