{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:24:49.528904Z",
     "start_time": "2020-03-25T15:24:46.761835Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pyodbc\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import tempfile\n",
    "from ttkthemes import ThemedStyle\n",
    "import tkinter as tk\n",
    "import tkinter.ttk as ttk\n",
    "from tkinter import messagebox, filedialog, commondialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:24:49.614873Z",
     "start_time": "2020-03-25T15:24:49.530876Z"
    }
   },
   "outputs": [],
   "source": [
    "class TISS_Metrics(): # {\n",
    "    \n",
    "    user_name = str(os.getlogin())\n",
    "    outbound_dir = \"C:/data/outbound/\" + str(pd.Timestamp.now())[:10]\n",
    "    desktop_dir = \"OneDrive - Agilent Technologies/Desktop\"\n",
    "    \n",
    "    def __init__(self, the_logger, materials_file): # {\n",
    "        self.the_logger = the_logger\n",
    "        self.materials_file = materials_file\n",
    "        # create dataframe from infile\n",
    "        self.df_infile = pd.read_csv(self.materials_file, header=0, dtype=np.str, engine='python')\n",
    "        # EXPORT?\n",
    "        self.df_infile.to_csv(os.path.join(self.outbound_dir, \"df_infile-\"\n",
    "                                          + str(pd.Timestamp.now())[:10]\n",
    "                                          + \".csv\"))\n",
    "        # Get/Set USERNAME & DESKTOP DIRECTORIES\n",
    "        self.user_name_dir = os.path.join(\"C:/Users/\", self.user_name)\n",
    "        self.desktop_dir = os.path.join(self.user_name_dir, self.desktop_dir)\n",
    "        print(self.user_name_dir)\n",
    "        print(self.desktop_dir)\n",
    "        self.run(day_range=30)\n",
    "    # }\n",
    "    \n",
    "    def run(self, day_range): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # [2020-02-28]\\\\self.time_unit = time_unit\n",
    "            # [2020-02-28]\\\\self.time_value = time_value\n",
    "            self.day_range = day_range\n",
    "            # get/set current date variable\n",
    "            # [2020-03-06]\\\\the_date = pd.Timestamp.now()\n",
    "            # [2020-03-11]\\\\the_date = pd.Timestamp(ts_input=str(self.end_date.get()))\n",
    "            the_date = pd.Timestamp.now()\n",
    "            # create variable for a month ago\n",
    "            one_month_ago = the_date - timedelta(days = int(day_range))\n",
    "            # [2020-02-28]\\\\\n",
    "            ##one_month_ago = the_date - pd.Timedelta(unit=str(self.time_unit), value=str(self.time_value))\n",
    "            print(type(one_month_ago))\n",
    "            # CREATE METRICS TABLE FROM CLASS METHOD\n",
    "            self.df_metrics_table = self.create_metrics_table()\n",
    "            #############################\n",
    "            # create .csv with no drops #\n",
    "            #############################\n",
    "            \"\"\"\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_metrics_noDrop.csv\"), \n",
    "                                         index=True)\n",
    "            # DROP ROWS WITH PRODUCT LEVEL && QCDATE = NONE\n",
    "            self.df_metrics_table.dropna(axis=0, subset=['QCDate', 'ProductLevel'], how='any', \n",
    "                                         inplace=True)\n",
    "            # create .csv with DROPS\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_metrics_DROP.csv\"), \n",
    "                                         index=True)\n",
    "            \"\"\"\n",
    "            # Set index of METRICS\n",
    "            # [2020-03-11]\\\\self.df_metrics_table.set_index(['QCDate'], inplace=True)\n",
    "            self.df_metrics_table.index = self.df_metrics_table['QCDate']\n",
    "            del self.df_metrics_table['QCDate']\n",
    "            print(self.df_metrics_table)\n",
    "            # Display Index\n",
    "            print(self.df_metrics_table.index)\n",
    "            #############################\n",
    "            # create .csv with UNSORTED #\n",
    "            #############################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_TIS_metrics_UNSORTED.csv\"), \n",
    "                                         index=True)\n",
    "            # SORT INDEX\n",
    "            self.df_metrics_table.sort_index(inplace=True)\n",
    "            ###########################\n",
    "            # create .csv with SORTED #\n",
    "            ###########################\n",
    "            self.df_metrics_table.to_csv(os.path.join(self.outbound_dir, \"df_TIS_metrics_SORTED.csv\"), \n",
    "                                         index=True)\n",
    "            print(\"SORTED:\\n\")\n",
    "            print(self.df_metrics_table.index)\n",
    "            # SLICE & DICE THE DATAFRAME\n",
    "            # [2020-03-11]\\\\self.df_level_1s = self.df_metrics_table.loc['2017-01-02']\n",
    "            print(\"<<<<< df_level_1s >>>>>\\n\\t\" + str(self.df_metrics_table.loc['2017-01-02':'2020-01-01'].describe()))\n",
    "            \"\"\"\n",
    "            <<< SLICE AND DICE THE DATAFRAME TO A MONTH PERIOD >>>< \n",
    "            \"\"\"\n",
    "            df_month = self.df_metrics_table.loc['2020-02-01':'2020-03-31']\n",
    "            # ***COPY DATAFRAME WITH RESET INDEX TO BE SAVED FOR LATER***\n",
    "            df_month_no_IDX = df_month.reset_index(drop=False)\n",
    "            # export\n",
    "            df_month_no_IDX.to_csv(os.path.join(self.outbound_dir, \"df_month_no_IDX-\"\n",
    "                                               + str(pd.Timestamp.now())[:10]\n",
    "                                               + \".csv\"), index_label=\"INDEX\", index=True)\n",
    "            print(\"df_month == (INDEX):\\n\" + str(df_month_no_IDX.index))\n",
    "            # CREATE INDEX VAR TO SAVE OLD INDEX (to be re-used)\n",
    "            qcdate_idx = df_month.index\n",
    "            print(\"qcdate_idx == (TYPE):\\n\" + str(type(qcdate_idx)))\n",
    "            print(\"LENGTH == \" + str(len(qcdate_idx)))\n",
    "            # SAVE QCDATE *COLUMN* TO BECOME REG COLUMN\n",
    "            qcdate_col = df_month_no_IDX['QCDate']\n",
    "            print(\"qcdate_col == (TYPE):\\n\" + str(type(qcdate_col)))\n",
    "            print(\"LENGTH == \" + str(len(qcdate_col)))\n",
    "            productno_col = df_month['ProductNo']\n",
    "            print('productno_col == (TYPE):\\n' + str(type(productno_col)))\n",
    "            print(\"LENGTH == \" + str(len(productno_col)))\n",
    "            # RE-ASSIGN OLD INDEX BACK IN AS COLUMN (of copied DataFrame)\n",
    "            df_month['QCDate'] = qcdate_col\n",
    "            # export\n",
    "            df_month.to_csv(os.path.join(self.outbound_dir, \"TIS-MONTH-\"\n",
    "                                        + str(pd.Timestamp.now())[:10]\n",
    "                                        + \".csv\"), index_label=\"INDEX\", index=True)\n",
    "            # CREATE EMPTY DATAFRAME TO (was... to HOLD THE MONTH OF FEBRUARY)\n",
    "            df_month_FINALE = pd.DataFrame(data=None, dtype=np.str, index=None) #[2020-04-01]\\\\df_month.index)\n",
    "            # [2020-03-31]\\\\df_month_FINALE[\"QCDate\"] = df_month.index\n",
    "            # [2020-04-01]\\\\df_month_FINALE[\"QCDate\"] = df_month.index\n",
    "            # [2020-04-01]\\\\df_month_FINALE['QCDate'] = qcdate_col # re-assign OLD INDEX to COLUMN\n",
    "            # NARROW DOWN ON COLUMNS\n",
    "            # [2020-04-03]\\\\df_month_FINALE['QCDate'] = qcdate_col # re-assign OLD INDEX to column\n",
    "            df_month_FINALE['QCDATE'] = self.df_tblProdflow['QCDate']\n",
    "            df_month_FINALE[\"OrderID\"] = df_month[\"OrderID\"]\n",
    "            df_month_FINALE[\"ProductNo\"] = df_month[\"ProductNo\"]\n",
    "            df_month_FINALE[\"Volume\"] = df_month[\"Volume\"]\n",
    "            df_month_FINALE[\"BuildByDate\"] = df_month[\"BuildByDate\"]\n",
    "            df_month_FINALE[\"OrderDate\"] = df_month[\"OrderDate\"]\n",
    "            df_month_FINALE[\"PrepDate\"] = df_month[\"PrepDate\"]\n",
    "            df_month_FINALE[\"PrepVolume\"] = df_month[\"PrepVolume\"]\n",
    "            df_month_FINALE[\"UnitizeNLT\"] = df_month[\"UnitizeNLT\"]\n",
    "            df_month_FINALE[\"ShipNLTDate\"] = df_month[\"ShipNLTDate\"]\n",
    "            df_month_FINALE[\"BulkQCDate\"] = df_month[\"BulkQCDate\"]\n",
    "            df_month_FINALE[\"AmpDate\"] = df_month[\"AmpDate\"]\n",
    "            df_month_FINALE[\"GreenSheet\"] = df_month[\"GreenSheet\"]\n",
    "            df_month_FINALE[\"SOLink\"] = df_month[\"SOLink\"]\n",
    "            df_month_FINALE[\"Notes\"] = df_month[\"Notes\"]\n",
    "            print(\"df_month_FINALE **INDEX**:\\n\" + str(df_month_FINALE.index))\n",
    "            # NARROW DOWN DATAFRAME TO ONLY CONTAIN CERTAIN DATES\n",
    "            # [2020-04-01]\\\\df_month_FINALE.loc['2020-02-01':'2020-03-31']\n",
    "            # Export?\n",
    "            df_month_FINALE.to_csv(os.path.join(self.outbound_dir, \"TIS-MONTH-FINALE-\"\n",
    "                                        + str(pd.Timestamp.now())[:10]\n",
    "                                        + \".csv\"), index_label=\"QCDate\", index=True)\n",
    "            df_merge = pd.merge(df_month_FINALE, self.df_infile, on='ProductNo', how='inner')\n",
    "            # EXPORT ??\n",
    "            df_merge.to_csv(os.path.join(self.outbound_dir, \"df-TIS-M-MERGE-\"\n",
    "                                        + str(pd.Timestamp.now())[:10]\n",
    "                                        + \".csv\"), index=True)\n",
    "            # CREATE DATAFRAME TO HOLD SAID INDEX (to be re-used)\n",
    "            s_month = pd.Series(data=None, index=qcdate_idx, name='QCDate') # data=df_month['ProductNo'], index=qcdate_idx\n",
    "            print(\"s_month:\\n\" + str(s_month))\n",
    "            df_save_qcdate = pd.DataFrame(data=productno_col, index=df_month.index) # df_month[\"ProductNo\"], index=df_month['QCDAte']\n",
    "            # RESET INDEX SO IT BECOME COLUMN\n",
    "            df_save_qcdate.reset_index(inplace=True)\n",
    "            # CONVERT FROM INDEX TO COLUMN\n",
    "            \n",
    "            # SET BACK AGAIN BUT KEEP QCDATE AS A COLUMN?\n",
    "            \n",
    "            print(\"\\tdf_save_qcdate:\\n\" + str(df_save_qcdate.info()))\n",
    "            df_save_qcdate.to_csv(os.path.join(self.outbound_dir, \"df-SAVE-qcdate-\"\n",
    "                                                  + str(pd.Timestamp.now())[:10]\n",
    "                                                  + \".csv\"), index=True)\n",
    "            print(str(df_save_qcdate))\n",
    "            # <<< MERGE INDEX OF QCDATE \"back\" INTO DATAFRAME? >>>\n",
    "            # >>> WITH SERIES CREATED ABOVE\n",
    "            df_merge_idx = df_merge.join(s_month, how='inner')\n",
    "            # [2020-04-03]\\\\df_merge_idx = df_merge.join(df_save_qcdate.set_index('QCDate'), on='QCDate') #, how='left')\n",
    "            # SET INDEX IN THE NEW MERGED FRAME SO WE GET BACK THE QCDATE\n",
    "            df_merge_idx.set_index(['QCDate'], inplace=True)\n",
    "            #df_merge_idx = df_merge.join(df_save_qcdate, on='ProductNo', how='inner')\n",
    "            \"\"\"\n",
    "            df_merge_idx = pd.concat(objs=[df_merge, df_save_qcdate], join=\"inner\", ) # , left_on='ProductNo', right_index=True)\n",
    "            df_merge_idx = pd.merge(left=df_merge, right=s_month, how=\"inner\", right_index=True)\n",
    "            \"\"\"\n",
    "            df_merge_idx.to_csv(os.path.join(self.outbound_dir, \"df_____idx-TIS-M-MERGE-\"\n",
    "                                            + str(pd.Timestamp.now())[:10]\n",
    "                                            + \".csv\"), index_label=\"INDEX\", index=True)\n",
    "            # << CHANGE INDEX BACK TO QCDATE? >>>\n",
    "            # [2020-04-01]\\\\df_merge.set_index(['QCDate'], drop=True)\n",
    "            # [2020-04-01]\\\\df_merge.set_index(qcdate_idx, drop=True, inplace=True) \n",
    "            # GET TIMESTMP FOR TODAY\n",
    "            ts_now = pd.Timestamp.now()\n",
    "            # create timedelta for LAST WEEK\n",
    "            td_last_week = pd.Timedelta(days=7)\n",
    "            # create timedelta for YESTERDAY\n",
    "            td_yesterday = pd.Timedelta(days=2)\n",
    "            # create actual TIMESTAMP for last week (whenever it was)\n",
    "            ts_last_week = pd.Timestamp(ts_now - td_last_week)\n",
    "            # create actual TIMESTAMP for YESTERDAY\n",
    "            ts_yesterday = pd.Timestamp(ts_now - td_yesterday)\n",
    "            ## CREATE DATAFRAME OF LAST WEEK\n",
    "            df_last_week = self.df_metrics_table.loc[ts_last_week:ts_now]\n",
    "            ## CREATE DATAFRAME OF YESTERDAY\n",
    "            df_yesterday = self.df_metrics_table.loc[ts_yesterday:ts_now]\n",
    "            # EXPORT ??\n",
    "            df_last_week.to_csv(os.path.join(self.outbound_dir, \"df_TIS-last-week-\" \n",
    "                                             + str(pd.Timestamp.now())[:10] \n",
    "                                             + \".csv\"))\n",
    "            print(\"LAST WEEK COLS:\\n\" + str(df_last_week.columns))\n",
    "            # CREATE NEW EMPTY DATAFRAME (to hold SPEICIFIC columns)\n",
    "            df_yesterday_FINALE = pd.DataFrame(data=None, dtype=np.str, index=df_yesterday.index)\n",
    "            # [2020-03-12]\\\\df_yesterday_FINALE[\"QCDate\"] = df_yesterday[\"QCDate\"]\n",
    "            # [2020-03-24]\\\\df_yesterday_FINALE[\"QCDate\"] = df_yesterday.index\n",
    "            df_yesterday_FINALE[\"QCDate\"] = self.df_tblProdflow[\"QCDate\"]\n",
    "            df_yesterday_FINALE[\"OrderID\"] = df_yesterday[\"OrderID\"]\n",
    "            df_yesterday_FINALE[\"ProductNo\"] = df_yesterday[\"ProductNo\"]\n",
    "            df_yesterday_FINALE[\"Volume\"] = df_yesterday[\"Volume\"]\n",
    "            df_yesterday_FINALE[\"BuildByDate\"] = df_yesterday[\"BuildByDate\"]\n",
    "            df_yesterday_FINALE[\"OrderDate\"] = df_yesterday[\"OrderDate\"]\n",
    "            df_yesterday_FINALE[\"PrepDate\"] = df_yesterday[\"PrepDate\"]\n",
    "            df_yesterday_FINALE[\"PrepVolume\"] = df_yesterday[\"PrepVolume\"]\n",
    "            # [2020-03-13]\\\\df_yesterday_FINALE[\"QCDate\"] = df_yesterday[\"QCDate\"]\n",
    "            # [2020-03-17]\\\\df_yesterday_FINALE[\"QCDate\"] = df_yesterday.index\n",
    "            df_yesterday_FINALE[\"UnitizeNLT\"] = df_yesterday[\"UnitizeNLT\"]\n",
    "            df_yesterday_FINALE[\"ShipNLTDate\"] = df_yesterday[\"ShipNLTDate\"]\n",
    "            df_yesterday_FINALE[\"BulkQCDate\"] = df_yesterday[\"BulkQCDate\"]\n",
    "            df_yesterday_FINALE[\"AmpDate\"] = df_yesterday[\"AmpDate\"]\n",
    "            df_yesterday_FINALE[\"GreenSheet\"] = df_yesterday[\"GreenSheet\"]\n",
    "            df_yesterday_FINALE[\"SOLink\"] = df_yesterday[\"SOLink\"]\n",
    "            # [2020-03-17]\\\\df_yesterday_FINALE[\"QCExpDate\"] = df_yesterday[\"QCExpDate\"]\n",
    "            # [2020-03-17]\\\\df_yesterday_FINALE[\"QCMemo\"] = df_yesterday[\"QCMemo\"]\n",
    "            df_yesterday_FINALE[\"Notes\"] = df_yesterday[\"Notes\"]\n",
    "            \"\"\"\n",
    "            df_yesterday_FINALE[\"PrepDate\"] = df_yesterday[\"PrepDate\"]\n",
    "            df_yesterday_FINALE[\"BulkQCDate\"] = df_yesterday[\"BulkQCDate\"]\n",
    "            df_yesterday_FINALE[\"AmpDate\"] = df_yesterday[\"AmpDate\"]\n",
    "            df_yesterday_FINALE[\"ProductNo\"] = df_yesterday[\"ProductNo\"]\n",
    "            df_yesterday_FINALE[\"OrderID\"] = df_yesterday[\"OrderID\"]\n",
    "            df_yesterday_FINALE[\"OrderDate\"] = df_yesterday[\"OrderDate\"]\n",
    "            df_yesterday_FINALE[\"Notes\"] = df_yesterday[\"Notes\"]\n",
    "            \"\"\"\n",
    "            df_yesterday_FINALE.to_csv(os.path.join(self.outbound_dir, \"df_TIS-yesterday-\"\n",
    "                                             + str(pd.Timestamp.now())[:10]\n",
    "                                             + \".csv\"), index=True)\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    \"\"\"\n",
    "    Referred to as \"ProdflowII\" in SQL-Server\n",
    "    \"\"\"\n",
    "    def pull_ProdflowII_table(self, table_name): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # CREATION CONNECTION STR\n",
    "            conn_str = str(\n",
    "                r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                r'SERVER=wtkngappflow1.is.agilent.net;'\n",
    "                r'DATABASE=ProdFlowII_Prod;'\n",
    "                r'Trusted_Connection=yes;'\n",
    "            )\n",
    "            # CREATE PYODBC CONNECTION\n",
    "            cnxn_ProdflowII = pyodbc.connect(conn_str)\n",
    "            # [2020-02-028]\\\\crsr_ProdflowII = cnxn_ProdflowII.cursor()\n",
    "            # PERFORM SQL QUERY AND SET AS DATAFRAME\n",
    "            df_ProdflowII_table = pd.read_sql_query(sql='SELECT * FROM ' + str(table_name),\n",
    "                                                    con=cnxn_ProdflowII\n",
    "                                                    )\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_ProdflowII_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    \"\"\"\n",
    "    Referred to as \"Prodflow\" in SQL-Server\n",
    "    \"\"\"\n",
    "    def pull_ProdflowIII_table(self, table_name): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # CREATE CONNECTION STRING\n",
    "            conn_str = str(\n",
    "                r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                r'SERVER=wtkngappflow1.is.agilent.net;'\n",
    "                r'DATABASE=ProdFlow;'\n",
    "                r'Trusted_Connection=yes;'\n",
    "            )\n",
    "            # CREATE PYODBC CONNECTION\n",
    "            cnxn_ProdflowIII = pyodbc.connect(conn_str)\n",
    "            # [2020-02-28]\\\\crsr_ProdflowIII = cnxn_ProdflowIII,cursor()\n",
    "            # PERFORM SQL QUERY AND SET AS DATAFRAME\n",
    "            df_ProdflowIII_table = pd.read_sql_query(sql='SELECT * FROM ' + str(table_name),\n",
    "                                                     con=cnxn_ProdflowIII,\n",
    "                                                     parse_dates=['QCDate']\n",
    "                                                     )\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        # }\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_ProdflowIII_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    def create_metrics_table(self): # {\n",
    "        # TRY THE FOLLOWING\n",
    "        try: # {\n",
    "            # pull ORDERS table\n",
    "            self.df_orders = self.pull_ProdflowII_table(table_name='Orders')\n",
    "            # EXPORT A SAMPLE\n",
    "            self.df_orders.sample(25).to_csv(os.path.join(self.outbound_dir, \"df-ORDERS-table-\"\n",
    "                                                         + str(pd.Timestamp.now())[:10]\n",
    "                                                         + \".csv\"))\n",
    "            # [2020-03-11]\\\\export_path = os.path.join(self.outbound_dir, \"df_orders.csv\")\n",
    "            # [2020-03-11]\\\\self.df_orders.to_csv(export_path, index=True)\n",
    "            # RENAME ['Product#'] column to [\"ProductNo\"]\n",
    "            self.df_orders.rename(columns={'Product#': 'ProductNo'}, inplace=True)\n",
    "            print(\"\\tORDERS-TABLE:\\n\" + str(self.df_orders.info()))\n",
    "            # pull tblProdflow TABLE\n",
    "            self.df_tblProdflow = self.pull_ProdflowIII_table(table_name='tblProdflow')\n",
    "            # EXPORT A SAMPLE\n",
    "            self.df_tblProdflow.sample(25).to_csv(os.path.join(self.outbound_dir, \"df-ORDERS-table-\"\n",
    "                                                         + str(pd.Timestamp.now())[:10]\n",
    "                                                         + \".csv\"))\n",
    "            print(self.df_tblProdflow.info())\n",
    "            ########################\n",
    "            # CREATE METRICS TABLE #\n",
    "            ########################\n",
    "            df_metrics_table = pd.merge(self.df_orders, self.df_tblProdflow, on='ProductNo', how='right')\n",
    "            ######################\n",
    "            # CHANGE DATA TYPES: #\n",
    "            ######################\n",
    "            \n",
    "            # CHANGE ['QCDate'] COLUMN TO DATETIME\n",
    "            df_metrics_table['QCDate'] = pd.to_datetime(df_metrics_table['QCDate'])\n",
    "            ####################################\n",
    "            # DROP ALL ROWS WITHOUT A 'QCDATE' #\n",
    "            ####################################\n",
    "            df_metrics_table.dropna(axis=0, subset=['QCDate'],\n",
    "                                   how='any', inplace=True)\n",
    "        # }\n",
    "        except: # {\n",
    "            errorMessage = str(sys.exc_info()[0]) + \"\\n\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[1]) + \"\\n\\t\\t\"\n",
    "            errorMessage = errorMessage + str(sys.exc_info()[2]) + \"\\n\"\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            typeE = str(\"TYPE : \" + str(exc_type))\n",
    "            fileE = str(\"FILE : \" + str(fname))\n",
    "            lineE = str(\"LINE : \" + str(exc_tb.tb_lineno))\n",
    "            messageE = str(\"MESG : \" + \"\\n\\n\" + str(errorMessage) + \"\\n\")\n",
    "            print(\"\\n\" + typeE + \n",
    "                  \"\\n\" + fileE + \n",
    "                  \"\\n\" + lineE + \n",
    "                  \"\\n\" + messageE)\n",
    "        #}\n",
    "        else: # {\n",
    "            print(\"Operation Completed Successfully...\")\n",
    "            return df_metrics_table\n",
    "        # }\n",
    "    # }\n",
    "    \n",
    "    def send_email(self, send_from, send_to, subject, message, files=[],\n",
    "                  server=\"cos.smtp.agilent.com\", port=587, use_tls=True): # {\n",
    "        print(\"SENDING E-MAIL...\\nDATE == \" + str(pd.Timestamp.now())[:10])\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = send_from\n",
    "        msg['To'] = COMMASPACE.join(send_to)\n",
    "        msg['Date'] = formatdate(localtime=True)\n",
    "        msg['Subject'] = subject\n",
    "        \n",
    "        msg.attach(MIMEText(message))\n",
    "    # }\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T15:28:26.151615Z",
     "start_time": "2020-03-25T15:24:49.616874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/derbates\n",
      "C:/Users/derbates\\OneDrive - Agilent Technologies/Desktop\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "Operation Completed Successfully...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 106890 entries, 0 to 106889\n",
      "Data columns (total 28 columns):\n",
      "OrderID            106890 non-null int64\n",
      "Quote#             106884 non-null object\n",
      "ProductNo          106890 non-null object\n",
      "LineID             106881 non-null float64\n",
      "Quantity           106821 non-null float64\n",
      "SaleUnit           95368 non-null object\n",
      "Company            106624 non-null object\n",
      "Volume             106620 non-null float64\n",
      "VolUnit            40636 non-null object\n",
      "OrderDate          106884 non-null datetime64[ns]\n",
      "BuildByDate        106329 non-null object\n",
      "ShipNLTDate        106295 non-null object\n",
      "Route              106889 non-null float64\n",
      "FullValidation     106890 non-null bool\n",
      "Validation         76436 non-null float64\n",
      "Built              106890 non-null bool\n",
      "GreenSheet         78631 non-null object\n",
      "Notes              32951 non-null object\n",
      "Completed          106890 non-null bool\n",
      "SOLink             32263 non-null object\n",
      "UserFormulation    32797 non-null object\n",
      "UserUnitizing      0 non-null object\n",
      "UserBulkQC         0 non-null object\n",
      "UserQC             0 non-null object\n",
      "EquipShort         2 non-null object\n",
      "UnitizeNLT         35677 non-null object\n",
      "SpecialPrep        106890 non-null bool\n",
      "RushPriority       106890 non-null bool\n",
      "dtypes: bool(5), datetime64[ns](1), float64(5), int64(1), object(16)\n",
      "memory usage: 19.3+ MB\n",
      "\tORDERS-TABLE:\n",
      "None\n",
      "Operation Completed Successfully...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109687 entries, 0 to 109686\n",
      "Data columns (total 53 columns):\n",
      "PfIDNo              109687 non-null int64\n",
      "PfBatchID           109687 non-null object\n",
      "ProductNo           109668 non-null object\n",
      "OrdID               93555 non-null float64\n",
      "QuoteNo             109651 non-null object\n",
      "PfSentTo            109622 non-null object\n",
      "PrepDate            108659 non-null datetime64[ns]\n",
      "PrepVolume          109642 non-null float64\n",
      "PrepUnit            109611 non-null object\n",
      "PrepVessel          109608 non-null float64\n",
      "PrepVBarcode        58998 non-null object\n",
      "PrdSaleUnit         90280 non-null object\n",
      "PrepMatrixNo        109612 non-null object\n",
      "PrepMatrixLot       108741 non-null object\n",
      "PrepInits           108500 non-null object\n",
      "PrepMemo            62055 non-null object\n",
      "PrepNotebookRef     48969 non-null object\n",
      "BulkQCStatus        24 non-null object\n",
      "BulkQCDate          75971 non-null datetime64[ns]\n",
      "BulkPassFail        74973 non-null object\n",
      "BulkQCInits         75955 non-null object\n",
      "BulkQCMemo          2231 non-null object\n",
      "BulkLotNo           74227 non-null object\n",
      "AmpRetains          52214 non-null float64\n",
      "AmpDate             93592 non-null datetime64[ns]\n",
      "AmpPreLabel         109687 non-null bool\n",
      "AmpNumberGood       93918 non-null float64\n",
      "AmpNumberBad        78102 non-null float64\n",
      "AmpBulkRemain       4691 non-null float64\n",
      "AmpTimeIn           93348 non-null datetime64[ns]\n",
      "AmpTimeOut          93348 non-null datetime64[ns]\n",
      "AmpInits            93556 non-null object\n",
      "QCStatus            573 non-null object\n",
      "QCDate              104075 non-null datetime64[ns]\n",
      "QCPassFail          104266 non-null object\n",
      "QCAuthInits         104064 non-null object\n",
      "QCMemo              55764 non-null object\n",
      "QCMethod            1209 non-null object\n",
      "LotNo               101356 non-null object\n",
      "QCExpMonth          94328 non-null float64\n",
      "QCExpDate           92730 non-null datetime64[ns]\n",
      "QCSellBy            0 non-null object\n",
      "QCFullValidation    109687 non-null bool\n",
      "QCValidation        109653 non-null float64\n",
      "FgInvCount          78785 non-null float64\n",
      "FgAccpacNote        19909 non-null object\n",
      "CofAHeaderNo        109655 non-null float64\n",
      "QCChromatogram      0 non-null object\n",
      "Correct             109687 non-null bool\n",
      "DoNotCorrect        109687 non-null bool\n",
      "DensityUnit         0 non-null object\n",
      "DensityTemp         0 non-null object\n",
      "recipeid            1 non-null object\n",
      "dtypes: bool(4), datetime64[ns](7), float64(11), int64(1), object(30)\n",
      "memory usage: 41.4+ MB\n",
      "None\n",
      "Operation Completed Successfully...\n",
      "            OrderID     Quote#    ProductNo  LineID  Quantity  \\\n",
      "QCDate                                                          \n",
      "2005-04-08      2.0  021004-27     CUS-5465     7.0      10.0   \n",
      "2005-04-08   3420.0  040405-54     CUS-5465     7.0       4.0   \n",
      "2004-04-13      2.0  021004-27     CUS-5465     7.0      10.0   \n",
      "2004-04-13   3420.0  040405-54     CUS-5465     7.0       4.0   \n",
      "2004-04-12      4.0  031504-11     CUS-5459     7.0      10.0   \n",
      "...             ...        ...          ...     ...       ...   \n",
      "2019-04-11      NaN        NaN  5190-0484-a     NaN       NaN   \n",
      "2019-10-04      NaN        NaN       BULK-2     NaN       NaN   \n",
      "2019-12-17      NaN        NaN      JHP-683     NaN       NaN   \n",
      "2020-02-05      NaN        NaN      JHP-327     NaN       NaN   \n",
      "2020-03-11      NaN        NaN      JHP-327     NaN       NaN   \n",
      "\n",
      "                                             SaleUnit  \\\n",
      "QCDate                                                  \n",
      "2005-04-08  1 mL in 2 mL Flame-Sealed Amber Ampule(s)   \n",
      "2005-04-08          1 mL Flame-Sealed Amber Ampule(s)   \n",
      "2004-04-13  1 mL in 2 mL Flame-Sealed Amber Ampule(s)   \n",
      "2004-04-13          1 mL Flame-Sealed Amber Ampule(s)   \n",
      "2004-04-12          1 mL Flame-Sealed Amber Ampule(s)   \n",
      "...                                               ...   \n",
      "2019-04-11                                        NaN   \n",
      "2019-10-04                                        NaN   \n",
      "2019-12-17                                        NaN   \n",
      "2020-02-05                                        NaN   \n",
      "2020-03-11                                        NaN   \n",
      "\n",
      "                               Company  Volume VolUnit  OrderDate  ...  \\\n",
      "QCDate                                                             ...   \n",
      "2005-04-08  Missouri American Water Co   850.0      mL 2014-05-05  ...   \n",
      "2005-04-08     Missouri American Water    25.0      mL 2005-04-05  ...   \n",
      "2004-04-13  Missouri American Water Co   850.0      mL 2014-05-05  ...   \n",
      "2004-04-13     Missouri American Water    25.0      mL 2005-04-05  ...   \n",
      "2004-04-12  Carnegie Mellon University    25.0      mL 2004-04-07  ...   \n",
      "...                                ...     ...     ...        ...  ...   \n",
      "2019-04-11                         NaN     NaN     NaN        NaT  ...   \n",
      "2019-10-04                         NaN     NaN     NaN        NaT  ...   \n",
      "2019-12-17                         NaN     NaN     NaN        NaT  ...   \n",
      "2020-02-05                         NaN     NaN     NaN        NaT  ...   \n",
      "2020-03-11                         NaN     NaN     NaN        NaT  ...   \n",
      "\n",
      "           QCValidation FgInvCount  FgAccpacNote CofAHeaderNo  QCChromatogram  \\\n",
      "QCDate                                                                          \n",
      "2005-04-08          1.0       16.0          None          1.0            None   \n",
      "2005-04-08          1.0       16.0          None          1.0            None   \n",
      "2004-04-13          1.0        NaN          None          1.0            None   \n",
      "2004-04-13          1.0        NaN          None          1.0            None   \n",
      "2004-04-12          1.0        NaN          None          1.0            None   \n",
      "...                 ...        ...           ...          ...             ...   \n",
      "2019-04-11          2.0       65.0          None          2.0            None   \n",
      "2019-10-04          2.0     2000.0    blue sheet          2.0            None   \n",
      "2019-12-17          1.0        NaN     no action         15.0            None   \n",
      "2020-02-05          1.0        NaN     no action         15.0            None   \n",
      "2020-03-11          1.0        NaN     no action         15.0            None   \n",
      "\n",
      "           Correct DoNotCorrect DensityUnit DensityTemp recipeid  \n",
      "QCDate                                                            \n",
      "2005-04-08   False         True        None        None     None  \n",
      "2005-04-08   False         True        None        None     None  \n",
      "2004-04-13   False         True        None        None     None  \n",
      "2004-04-13   False         True        None        None     None  \n",
      "2004-04-12   False         True        None        None     None  \n",
      "...            ...          ...         ...         ...      ...  \n",
      "2019-04-11   False        False        None        None     None  \n",
      "2019-10-04   False        False        None        None     None  \n",
      "2019-12-17   False         True        None        None     None  \n",
      "2020-02-05   False         True        None        None     None  \n",
      "2020-03-11   False         True        None        None     None  \n",
      "\n",
      "[1818774 rows x 79 columns]\n",
      "DatetimeIndex(['2005-04-08', '2005-04-08', '2004-04-13', '2004-04-13',\n",
      "               '2004-04-12', '2012-03-14', '2012-03-14', '2012-03-14',\n",
      "               '2012-03-14', '2012-03-14',\n",
      "               ...\n",
      "               '2019-05-01', '2019-02-01', '2019-07-03', '2019-02-19',\n",
      "               '2019-03-04', '2019-04-11', '2019-10-04', '2019-12-17',\n",
      "               '2020-02-05', '2020-03-11'],\n",
      "              dtype='datetime64[ns]', name='QCDate', length=1818774, freq=None)\n",
      "SORTED:\n",
      "\n",
      "DatetimeIndex(['1987-01-01', '1987-01-01', '1989-01-01', '1991-01-01',\n",
      "               '1991-01-01', '1991-01-01', '1991-01-01', '1992-01-01',\n",
      "               '1992-01-01', '1992-01-01',\n",
      "               ...\n",
      "               '2020-04-10', '2020-04-10', '2020-04-10', '2020-04-10',\n",
      "               '2020-04-10', '2020-04-10', '2020-04-10', '2020-04-10',\n",
      "               '2020-04-10', '2020-04-10'],\n",
      "              dtype='datetime64[ns]', name='QCDate', length=1818774, freq=None)\n",
      "<<<<< df_level_1s >>>>>\n",
      "\t             OrderID         LineID       Quantity        Volume     Route  \\\n",
      "count  490344.000000  490341.000000  490335.000000  4.897690e+05  490324.0   \n",
      "mean    66844.811496       9.114993       3.451793  6.033272e+04       1.0   \n",
      "std     29425.288141       5.277513      63.798720  3.005593e+05       0.0   \n",
      "min         7.000000       0.000000       0.000000  0.000000e+00       1.0   \n",
      "25%     43263.000000       7.000000       0.000000  2.500000e+02       1.0   \n",
      "50%     70760.000000       7.000000       0.000000  2.000000e+04       1.0   \n",
      "75%     93232.000000       8.000000       0.000000  2.000000e+04       1.0   \n",
      "max    109016.000000      22.000000   11000.000000  8.640000e+06       1.0   \n",
      "\n",
      "          Validation         PfIDNo          OrdID    PrepVolume  \\\n",
      "count  421824.000000  491469.000000  491420.000000  4.914690e+05   \n",
      "mean        0.000002  140211.467352   93508.508536  6.163499e+04   \n",
      "std         0.001540    7812.569172    8317.244151  2.987093e+05   \n",
      "min         0.000000     996.000000       0.000000  1.000000e-02   \n",
      "25%         0.000000  134163.000000   87389.000000  2.500000e+02   \n",
      "50%         0.000000  140875.000000   94326.000000  2.000000e+04   \n",
      "75%         0.000000  146419.000000  100143.000000  2.000000e+04   \n",
      "max         1.000000  152150.000000  105861.000000  2.000000e+06   \n",
      "\n",
      "          PrepVessel    AmpRetains  AmpNumberGood   AmpNumberBad  \\\n",
      "count  491469.000000  79872.000000   4.647370e+05  256565.000000   \n",
      "mean        2.195935      4.732560   1.403700e+03       4.425288   \n",
      "std         0.999591      1.086707   4.148238e+04      17.788871   \n",
      "min         1.000000      0.000000   0.000000e+00       0.000000   \n",
      "25%         1.000000      5.000000   4.200000e+01       0.000000   \n",
      "50%         3.000000      5.000000   4.300000e+01       0.000000   \n",
      "75%         3.000000      5.000000   4.300000e+01       5.000000   \n",
      "max         4.000000     15.000000   2.000000e+06    1100.000000   \n",
      "\n",
      "       AmpBulkRemain     QCExpMonth   QCValidation    FgInvCount  \\\n",
      "count   18457.000000  491464.000000  491469.000000  4.647430e+05   \n",
      "mean      666.251178      34.603969       1.648377  1.403509e+03   \n",
      "std      3634.536605      17.110797       0.503783  4.148199e+04   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%        40.000000      24.000000       1.000000  4.200000e+01   \n",
      "50%        70.000000      48.000000       2.000000  4.300000e+01   \n",
      "75%       350.000000      48.000000       2.000000  4.300000e+01   \n",
      "max     60000.000000     120.000000       3.000000  2.000000e+06   \n",
      "\n",
      "        CofAHeaderNo  \n",
      "count  491469.000000  \n",
      "mean        2.313910  \n",
      "std         2.727627  \n",
      "min         1.000000  \n",
      "25%         1.000000  \n",
      "50%         2.000000  \n",
      "75%         2.000000  \n",
      "max        15.000000  \n",
      "df_month == (INDEX):\n",
      "RangeIndex(start=0, stop=42996, step=1)\n",
      "qcdate_idx == (TYPE):\n",
      "<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "LENGTH == 42996\n",
      "qcdate_col == (TYPE):\n",
      "<class 'pandas.core.series.Series'>\n",
      "LENGTH == 42996\n",
      "productno_col == (TYPE):\n",
      "<class 'pandas.core.series.Series'>\n",
      "LENGTH == 42996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derbates\\miniconda3\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TYPE : <class 'ValueError'>\n",
      "FILE : <ipython-input-2-4d6719383f6c>\n",
      "LINE : 114\n",
      "MESG : \n",
      "\n",
      "<class 'ValueError'>\n",
      "cannot reindex from a duplicate axis\n",
      "\t\t<traceback object at 0x000001C679B72308>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": # {\n",
    "    test_metrics = TISS_Metrics(the_logger=None, \n",
    "                                materials_file=\"C:/data/inbound/2020-04-01/material_list.csv\")\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
